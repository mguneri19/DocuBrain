import os
import streamlit as st
import pandas as pd

# Streamlit config
st.set_page_config(
    page_title="DocuBrain - Intelligent Document Assistant",
    layout="wide"
)

# Basic UI
st.title("ğŸ§  DocuBrain")
st.markdown("*Turn your PDFs and Docs into an intelligent assistant.*", help="DocuBrain ile dokÃ¼manlarÄ±nÄ±zÄ± akÄ±llÄ± asistanÄ±nÄ±za dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n")

# Check basic imports
try:
    from langchain_openai import ChatOpenAI
    st.success("âœ… LangChain OpenAI import baÅŸarÄ±lÄ±")
except Exception as e:
    st.error(f"âŒ LangChain OpenAI import hatasÄ±: {e}")
    st.stop()

# Check config
try:
    from config import OPENAI_API_KEY, DEFAULT_OPENAI_MODEL
    st.success("âœ… Config import baÅŸarÄ±lÄ±")
except Exception as e:
    st.error(f"âŒ Config import hatasÄ±: {e}")
    st.stop()

# Check API key
if OPENAI_API_KEY:
    st.success("âœ… OPENAI_API_KEY bulundu")
else:
    st.error("âŒ OPENAI_API_KEY bulunamadÄ±")
    st.stop()

# Initialize session state
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "chat_history_chain" not in st.session_state:
    st.session_state.chat_history_chain = []
if "chat_history_agent" not in st.session_state:
    st.session_state.chat_history_agent = []
if "agent_exec" not in st.session_state:
    st.session_state.agent_exec = None
if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = []
if "indexed_files" not in st.session_state:
    st.session_state.indexed_files = []

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    
    # Mode selection
    mode = st.selectbox(
        "KullanÄ±m Modu",
        ["RAG Chain", "Agent (tools)"],
        help="RAG Chain: Basit soru-cevap, Agent: GeliÅŸmiÅŸ araÃ§lar"
    )
    
    # Answer style
    answer_style = st.radio(
        "Cevap Stili",
        ["KÄ±sa ve Ã–z", "DetaylÄ± ve KapsamlÄ±"],
        help="Cevap uzunluÄŸunu kontrol eder"
    )
    
    # LLM selection
    llm_provider = st.radio(
        "LLM SaÄŸlayÄ±cÄ±sÄ±",
        ["OpenAI", "Ollama (Fallback)"],
        help="Hangi LLM saÄŸlayÄ±cÄ±sÄ±nÄ± kullanacaÄŸÄ±nÄ±zÄ± seÃ§in"
    )
    
    # Reset database
    if st.button("ğŸ—‘ï¸ Veri TabanÄ±nÄ± SÄ±fÄ±rla", help="TÃ¼m indekslenmiÅŸ dosyalarÄ± siler"):
        st.session_state.vectorstore = None
        st.session_state.retriever = None
        st.session_state.chat_history_chain = []
        st.session_state.chat_history_agent = []
        st.session_state.agent_exec = None
        st.session_state.uploaded_files = []
        st.session_state.indexed_files = []
        st.success("âœ… Veri tabanÄ± sÄ±fÄ±rlandÄ±!")
        st.rerun()

# Main tabs
tab1, tab2, tab3 = st.tabs(["ğŸ“š Knowledge Base", "ğŸ’¬ Chat", "ğŸ“Š Logs"])

with tab1:
    st.write("**Dosya YÃ¼kleme**")
    
    # Show indexed files if any
    if st.session_state.indexed_files:
        st.success(f"ğŸ“š **Ä°ndekslenen Dosyalar:** {len(st.session_state.indexed_files)}")
        
        # File deletion
        files_to_delete = st.multiselect(
            "Silinecek dosyalarÄ± seÃ§in:",
            st.session_state.indexed_files,
            help="SeÃ§ili dosyalarÄ± silmek iÃ§in kullanÄ±n"
        )
        
        if st.button("ğŸ—‘ï¸ SeÃ§ili DosyalarÄ± Sil", disabled=not files_to_delete):
            try:
                from config import UPLOAD_DIRECTORY
                import os
                
                # Delete selected files
                for file_name in files_to_delete:
                    file_path = os.path.join(UPLOAD_DIRECTORY, file_name)
                    if os.path.exists(file_path):
                        os.remove(file_path)
                
                # Update session state
                st.session_state.indexed_files = [f for f in st.session_state.indexed_files if f not in files_to_delete]
                
                # Reset vectorstore if no files left
                if not st.session_state.indexed_files:
                    st.session_state.vectorstore = None
                    st.session_state.retriever = None
                
                st.success(f"âœ… {len(files_to_delete)} dosya silindi!")
                st.rerun()
                
            except Exception as e:
                st.error(f"âŒ Silme hatasÄ±: {str(e)}")
        
        st.divider()
    
    uploaded_files = st.file_uploader(
        "PDF veya DOCX dosyalarÄ±nÄ±zÄ± yÃ¼kleyin",
        type=['pdf', 'docx'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.success(f"âœ… {len(uploaded_files)} dosya yÃ¼klendi")
        for file in uploaded_files:
            st.write(f"- {file.name}")
        
        if st.button("ğŸ“ Ä°ndeksle", help="DosyalarÄ± vektÃ¶r veritabanÄ±na ekler"):
            st.info("ğŸ”„ Ä°ndeksleme iÅŸlemi baÅŸlatÄ±lÄ±yor...")
            
            # Try to import indexing modules
            try:
                from ingest import index_files, get_vectorstore
                from config import UPLOAD_DIRECTORY
                from utils import ensure_dirs
                
                # Ensure directories exist
                ensure_dirs()
                
                # Save uploaded files
                saved_paths = []
                for uploaded_file in uploaded_files:
                    # Create upload directory if it doesn't exist
                    os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)
                    
                    # Save file
                    file_path = os.path.join(UPLOAD_DIRECTORY, uploaded_file.name)
                    with open(file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    saved_paths.append(file_path)
                
                # Update session state
                st.session_state.uploaded_files = saved_paths
                
                # Index files
                with st.spinner("ğŸ“š Dosyalar indeksleniyor..."):
                    try:
                        # Convert string paths to Path objects
                        from pathlib import Path
                        path_objects = [Path(p) for p in saved_paths]
                        result = index_files(path_objects)
                        if result and len(result) == 2:
                            docs_count, chunks_count = result
                            st.session_state.vectorstore = get_vectorstore()
                            st.session_state.indexed_files = [os.path.basename(path) for path in saved_paths]
                            st.success(f"âœ… {docs_count} dosya, {chunks_count} chunk baÅŸarÄ±yla indekslendi!")
                            
                            # Logging removed for simplicity
                        else:
                            st.error("âŒ Ä°ndeksleme baÅŸarÄ±sÄ±z!")
                    except Exception as e:
                        st.error(f"âŒ Ä°ndeksleme hatasÄ±: {str(e)}")
                        
            except ImportError as e:
                st.error(f"âŒ Import hatasÄ±: {str(e)}")
                st.warning("âš ï¸ Ä°ndeksleme modÃ¼lleri yÃ¼klenemedi. Ana uygulamayÄ± kullanÄ±n.")
            except Exception as e:
                st.error(f"âŒ Genel hata: {str(e)}")

with tab2:
    st.write("**Sohbet**")
    
    # Check if files are indexed
    if not st.session_state.indexed_files:
        st.warning("âš ï¸ Ã–nce dosyalarÄ± indekslemeniz gerekiyor. Knowledge Base sekmesine gidin.")
    else:
        st.success(f"ğŸ“š **{len(st.session_state.indexed_files)} dosya indekslendi**")
    
    # Chat input
    question = st.text_input("Sorunuzu yazÄ±n:", placeholder="DokÃ¼manlarÄ±nÄ±z hakkÄ±nda soru sorun...")
    
    if st.button("GÃ¶nder") and question:
        st.write(f"**Soru:** {question}")
        
        if not st.session_state.indexed_files:
            st.error("âŒ Ã–nce dosyalarÄ± indekslemeniz gerekiyor!")
        else:
            try:
                # Import RAG modules
                from rag_chain import build_retriever, answer_with_chain
                from langchain_openai import ChatOpenAI
                from langchain_core.messages import AIMessage, HumanMessage
                
                # Initialize LLM
                llm = ChatOpenAI(
                    model=DEFAULT_OPENAI_MODEL,
                    api_key=OPENAI_API_KEY,
                    temperature=0.7
                )
                
                # Build retriever if not exists
                if not st.session_state.retriever:
                    st.session_state.retriever = build_retriever(st.session_state.vectorstore)
                
                # Determine answer style
                is_short = (answer_style == "KÄ±sa ve Ã–z")
                
                # Get answer
                with st.spinner("ğŸ¤” Cevap oluÅŸturuluyor..."):
                    if mode == "RAG Chain":
                        result = answer_with_chain(
                            st.session_state.retriever,
                            llm,
                            question,
                            st.session_state.chat_history_chain,
                            is_short
                        )
                        
                        if result and "answer" in result:
                            answer = result["answer"]
                            st.write("**Cevap:**", answer)
                            
                            # Update chat history
                            st.session_state.chat_history_chain.append(HumanMessage(content=question))
                            st.session_state.chat_history_chain.append(AIMessage(content=answer))
                            
                            # Show token usage if available
                            if "tokens" in result:
                                tokens = result["tokens"]
                                st.caption(f"Token kullanÄ±mÄ±: {tokens.get('total_tokens', 'N/A')} | Maliyet: ${tokens.get('total_cost', 'N/A')}")
                            
                            # Logging removed for simplicity
                        else:
                            st.error("âŒ Cevap oluÅŸturulamadÄ±!")
                    
                    elif mode == "Agent (tools)":
                        # Import agent modules
                        from agent import build_agent, run_agent
                        
                        # Build agent if not exists
                        if not st.session_state.agent_exec:
                            st.session_state.agent_exec = build_agent(
                                st.session_state.retriever,
                                llm,
                                is_short
                            )
                        
                        result = run_agent(
                            st.session_state.agent_exec,
                            question,
                            st.session_state.chat_history_agent
                        )
                        
                        if result and "answer" in result:
                            answer = result["answer"]
                            st.write("**Cevap:**", answer)
                            
                            # Update chat history
                            st.session_state.chat_history_agent.append(HumanMessage(content=question))
                            st.session_state.chat_history_agent.append(AIMessage(content=answer))
                            
                            # Show token usage if available
                            if "tokens" in result:
                                tokens = result["tokens"]
                                st.caption(f"Token kullanÄ±mÄ±: {tokens.get('total_tokens', 'N/A')} | Maliyet: ${tokens.get('total_cost', 'N/A')}")
                            
                            # Logging removed for simplicity
                        else:
                            st.error("âŒ Agent cevap oluÅŸturamadÄ±!")
                
            except ImportError as e:
                st.error(f"âŒ Import hatasÄ±: {str(e)}")
                st.warning("âš ï¸ RAG modÃ¼lleri yÃ¼klenemedi. Ana uygulamayÄ± kullanÄ±n.")
            except Exception as e:
                st.error(f"âŒ Chat hatasÄ±: {str(e)}")
    
    # Show chat history
    if st.session_state.chat_history_chain or st.session_state.chat_history_agent:
        st.divider()
        st.write("**Sohbet GeÃ§miÅŸi:**")
        
        # Show appropriate chat history based on mode
        chat_history = st.session_state.chat_history_chain if mode == "RAG Chain" else st.session_state.chat_history_agent
        
        for i, message in enumerate(chat_history):
            if isinstance(message, HumanMessage):
                st.write(f"**ğŸ‘¤ Soru {i//2 + 1}:** {message.content}")
            elif isinstance(message, AIMessage):
                st.write(f"**ğŸ¤– Cevap {i//2 + 1}:** {message.content}")
                st.divider()
        
        # Clear chat button
        if st.button("ğŸ—‘ï¸ Sohbeti Temizle"):
            if mode == "RAG Chain":
                st.session_state.chat_history_chain = []
            else:
                st.session_state.chat_history_agent = []
            st.rerun()

with tab3:
    st.write("**KullanÄ±cÄ± Aktivite LoglarÄ±**")
    
    st.info("ğŸ“Š **Logs sekmesi kaldÄ±rÄ±ldÄ±** - Uygulama daha hafif hale getirildi")

st.info("ğŸ¯ **Working DocuBrain** - Ana Ã¶zellikler yavaÅŸ yavaÅŸ eklenecek")

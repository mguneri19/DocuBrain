import os
from pathlib import Path
import streamlit as st
import pandas as pd

# Lazy imports - sadece gerektiÄŸinde yÃ¼kle
def get_imports():
    """Lazy imports to reduce startup time"""
    try:
        from langchain_openai import ChatOpenAI
        from langchain_community.chat_models import ChatOllama
        from langchain_core.messages import AIMessage, HumanMessage
        return True, None
    except Exception as e:
        return False, str(e)

def get_rag_imports():
    """Lazy imports for RAG functionality"""
    try:
        from ingest import index_files, get_vectorstore, reset_vectorstore
        from rag_chain import build_retriever, answer_with_chain
        from agent import build_agent, run_agent
        return True, None
    except Exception as e:
        return False, str(e)

def get_config():
    """Lazy config import"""
    try:
        from config import (
            UPLOAD_DIRECTORY, PERSIST_DIRECTORY,
            OPENAI_API_KEY, DEFAULT_OPENAI_MODEL,
            SEARCH_TYPE, TOP_K, MMR_LAMBDA
        )
        from rag_chain import ensure_dirs
        return True, None
    except Exception as e:
        return False, str(e)

st.set_page_config(page_title="DocuBrain - Intelligent Document Assistant", layout="wide")
st.title("ğŸ§  DocuBrain")
st.markdown("*Turn your PDFs and Docs into an intelligent assistant.*", help="DocuBrain ile dokÃ¼manlarÄ±nÄ±zÄ± akÄ±llÄ± asistanÄ±nÄ±za dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n")

# Check imports
import_success, import_error = get_imports()
if not import_success:
    st.error(f"âŒ Import hatasÄ±: {import_error}")
    st.stop()

rag_success, rag_error = get_rag_imports()
if not rag_success:
    st.error(f"âŒ RAG import hatasÄ±: {rag_error}")
    st.stop()

config_success, config_error = get_config()
if not config_success:
    st.error(f"âŒ Config import hatasÄ±: {config_error}")
    st.stop()

# Import everything now that we know it works
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import AIMessage, HumanMessage
from ingest import index_files, get_vectorstore, reset_vectorstore
from rag_chain import build_retriever, answer_with_chain
from agent import build_agent, run_agent
# Logger removed for simplicity
from config import (
    UPLOAD_DIRECTORY, PERSIST_DIRECTORY,
    OPENAI_API_KEY, DEFAULT_OPENAI_MODEL,
    SEARCH_TYPE, TOP_K, MMR_LAMBDA
)
from rag_chain import ensure_dirs
from chat_storage import save_chat_history, load_chat_history, clear_chat_history

# State
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "retriever" not in st.session_state:
    st.session_state.retriever = None
# Chat history will be loaded from file storage
if "chat_history_chain" not in st.session_state:
    st.session_state.chat_history_chain = load_chat_history("rag_chain")
if "chat_history_agent" not in st.session_state:
    st.session_state.chat_history_agent = load_chat_history("agent")
if "agent_exec" not in st.session_state:
    st.session_state.agent_exec = None
if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = []
if "indexed_files" not in st.session_state:
    st.session_state.indexed_files = []

ensure_dirs(UPLOAD_DIRECTORY, PERSIST_DIRECTORY)

# Sidebar â€” settings
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    # Mod: OpenAI anahtarÄ± varsa varsayÄ±lan olarak Agent; yoksa RAG Chain
    mode_default = "Agent (tools)" if OPENAI_API_KEY else "RAG Chain"
    mode = st.selectbox(
        "KullanÄ±m Modu",
        ["RAG Chain", "Agent (tools)"],
        index=1 if mode_default == "Agent (tools)" else 0,
        help="RAG Chain: Basit ve hÄ±zlÄ±. Agent: Her soruda bilgi tabanÄ± aracÄ±nÄ± kullanÄ±r (OpenAI Ã¶nerilir).",
    )

    # Optimized settings - kullanÄ±cÄ± ayarlamasÄ±na gerek yok
    top_k = TOP_K  # 8 - optimal chunk sayÄ±sÄ±
    search_type = SEARCH_TYPE  # "mmr" - Ã§eÅŸitlilik iÃ§in
    mmr_lambda = float(MMR_LAMBDA)  # 0.6 - optimal denge
    
    
    # Cevap uzunluÄŸu seÃ§imi
    st.divider()
    st.subheader("ğŸ’¬ Cevap Stili")
    answer_style = st.radio(
        "Cevap uzunluÄŸu seÃ§in:",
        ["KÄ±sa ve Ã–z", "DetaylÄ± ve KapsamlÄ±"],
        index=0,
        help="KÄ±sa: 2-3 cÃ¼mle, DetaylÄ±: KapsamlÄ± aÃ§Ä±klama"
    )

    st.divider()
    st.caption("LLM SeÃ§imi")
    
    # Sadece OpenAI kullanÄ±labilir
    use_openai = True
    if not OPENAI_API_KEY:
        st.info("OPENAI_API_KEY bulunamadÄ±. .env dosyasÄ±yla ekleyebilirsiniz.")
    # Sadece gpt-4o-mini kullanÄ±labilir (maliyet optimizasyonu)
    openai_model = "gpt-4o-mini"
    st.info("ğŸš€ **GPT-4o-mini** seÃ§ildi (hÄ±zlÄ± + ekonomik)")
    st.caption("OpenAI GPT-4o-mini modeli kullanÄ±lÄ±yor.")

    if st.button("ğŸ—‘ï¸ Veri TabanÄ±nÄ± SÄ±fÄ±rla"):
        reset_vectorstore()
        st.session_state.vectorstore = None
        st.session_state.retriever = None
        st.session_state.agent_exec = None
        st.success("Chroma veritabanÄ± sÄ±fÄ±rlandÄ±.")
    

# Tabs
tab_kb, tab_chat = st.tabs(["ğŸ“ Knowledge Base", "ğŸ’¬ Chat"])

with tab_kb:
    st.subheader("Dosya YÃ¼kle (PDF / DOCX)")
    
    # Sayfa yÃ¼klendiÄŸinde mevcut dosyalarÄ± kontrol et
    if not st.session_state.indexed_files:
        existing_files = [p for p in UPLOAD_DIRECTORY.iterdir() if p.suffix.lower() in [".pdf", ".docx"]]
        if existing_files:
            st.session_state.indexed_files = existing_files
            # EÄŸer vectorstore varsa, dosyalar zaten indekslenmiÅŸ demektir
            if st.session_state.vectorstore is None:
                try:
                    st.session_state.vectorstore = get_vectorstore()
                    st.session_state.retriever = build_retriever(
                        st.session_state.vectorstore,
                        search_type=SEARCH_TYPE,
                        top_k=TOP_K,
                        mmr_lambda=MMR_LAMBDA,
                    )
                except Exception:
                    pass  # Vectorstore yoksa yeni indeksleme gerekir
    
    files = st.file_uploader("DosyalarÄ± seÃ§in", type=["pdf", "docx"], accept_multiple_files=True)
    if files:
        saved_paths = []
        for f in files:
            save_path = UPLOAD_DIRECTORY / f.name
            with open(save_path, "wb") as out:
                out.write(f.read())
            saved_paths.append(save_path)
        st.session_state.uploaded_files = saved_paths
        st.success(f"{len(saved_paths)} dosya yÃ¼klendi.")
        
        # Logging removed for simplicity

    if st.button("ğŸ“¥ Ä°ndeksle (Chroma'ya ekle)"):
        path_list = [p for p in UPLOAD_DIRECTORY.iterdir() if p.suffix.lower() in [".pdf", ".docx"]]
        if not path_list:
            st.warning("Ã–nce en az bir PDF/DOCX yÃ¼kleyin.")
        else:
            try:
                import time
                start_t = time.time()
                with st.spinner("Belgeler indeksleniyor, lÃ¼tfen bekleyinâ€¦"):
                    raw_n, chunk_n = index_files(path_list)
                elapsed = time.time() - start_t
                st.success(f"YÃ¼klendi: {raw_n} belge, {chunk_n} parÃ§a eklendi. ({elapsed:.1f}s)")
                st.session_state.vectorstore = get_vectorstore()
                st.session_state.retriever = build_retriever(
                    st.session_state.vectorstore,
                    search_type=search_type,
                    top_k=top_k,
                    mmr_lambda=mmr_lambda,
                )
                st.session_state.agent_exec = None  # rebuild agent next time
                st.session_state.indexed_files = path_list  # Store indexed files
                
                # Logging removed for simplicity
            except OSError as e:
                if getattr(e, 'errno', None) == 28:
                    st.error(
                        "Disk dolu: HuggingFace modeli indirilirken yer kalmadÄ±. \n\n"
                        "Ã‡Ã¶zÃ¼m: Daha boÅŸ bir dizine cache yÃ¶nlendirin (Ã¶rn. D:):\n"
                        "- PowerShell (geÃ§ici): $env:HF_HOME=\"D:\\hf_cache\"\n"
                        "- KalÄ±cÄ±: Sistem DeÄŸiÅŸkeni olarak HF_HOME veya HUGGINGFACE_HUB_CACHE ekleyin.\n"
                        "- Alternatif: .env iÃ§ine EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2 yazÄ±n (daha kÃ¼Ã§Ã¼k model).\n"
                        "- Veya disk alanÄ± boÅŸaltÄ±n."
                    )
                else:
                    st.error(f"Ä°ndeksleme sÄ±rasÄ±nda hata: {e}")

    # Show uploaded files info
    st.divider()
    st.subheader("ğŸ“Š Dosya Durumu")
    
    if st.session_state.indexed_files:
        st.success(f"âœ… **Ä°ndekslenen dosyalar:** {len(st.session_state.indexed_files)} dosya")
        
        # Tablo formatÄ±nda gÃ¶ster
        file_data = []
        for file_path in st.session_state.indexed_files:
            file_size = os.path.getsize(file_path) / 1024  # KB
            file_data.append({
                "Dosya AdÄ±": file_path.name,
                "Tip": file_path.suffix.upper(),
                "Boyut (KB)": f"{file_size:.1f}"
            })
        
        if file_data:
            df = pd.DataFrame(file_data)
            st.dataframe(df, width="stretch", hide_index=True)
            
            # Dosya silme seÃ§eneÄŸi
            st.divider()
            col1, col2 = st.columns([3, 1])
            with col1:
                selected_files = st.multiselect(
                    "Silmek istediÄŸiniz dosyalarÄ± seÃ§in:",
                    options=[f.name for f in st.session_state.indexed_files],
                    help="Birden fazla dosya seÃ§ebilirsiniz"
                )
            with col2:
                st.write("")  # BoÅŸluk iÃ§in
                if st.button("ğŸ—‘ï¸ SeÃ§ili DosyalarÄ± Sil", type="secondary", disabled=not selected_files):
                    # SeÃ§ili dosyalarÄ± sil
                    for file_name in selected_files:
                        for file_path in st.session_state.indexed_files:
                            if file_path.name == file_name:
                                try:
                                    # DosyayÄ± fiziksel olarak sil
                                    file_path.unlink()
                                    st.success(f"âœ… {file_name} silindi")
                                except Exception as e:
                                    st.error(f"âŒ {file_name} silinemedi: {e}")
                    
                    # Session state'i gÃ¼ncelle
                    st.session_state.indexed_files = [
                        f for f in st.session_state.indexed_files 
                        if f.name not in selected_files
                    ]
                    
                    # EÄŸer tÃ¼m dosyalar silindiyse vectorstore'u temizle
                    if not st.session_state.indexed_files:
                        reset_vectorstore()
                        st.session_state.vectorstore = None
                        st.session_state.retriever = None
                        st.session_state.agent_exec = None
                        st.info("TÃ¼m dosyalar silindi. Veri tabanÄ± sÄ±fÄ±rlandÄ±.")
                    else:
                        # Kalan dosyalar varsa yeniden indeksle
                        st.warning("âš ï¸ Dosyalar silindi. DeÄŸiÅŸikliklerin etkili olmasÄ± iÃ§in 'Ä°ndeksle' butonuna tÄ±klayÄ±n.")
                    
                    st.rerun()
    else:
        st.info("ğŸ“ HenÃ¼z indekslenmiÅŸ dosya yok. Dosya yÃ¼kleyip 'Ä°ndeksle' butonuna tÄ±klayÄ±n.")
    
    if st.session_state.uploaded_files:
        st.caption(f"Son yÃ¼kleme: {len(st.session_state.uploaded_files)} dosya")
    
    st.caption(f"ğŸ’¾ Chroma klasÃ¶rÃ¼: {PERSIST_DIRECTORY}")

with tab_chat:
    col1, col2 = st.columns([3, 1])
    with col1:
        st.subheader("Sohbet")
    with col2:
        if st.button("ğŸ—‘ï¸ Sohbeti Temizle", help="TÃ¼m sohbet geÃ§miÅŸini sil"):
            clear_chat_history()  # TÃ¼m sohbet geÃ§miÅŸini temizle
            st.session_state.chat_history_chain = []
            st.session_state.chat_history_agent = []
            st.success("âœ… Sohbet geÃ§miÅŸi temizlendi!")
            st.rerun()
    
    # Sohbet yÃ¶netimi kaldÄ±rÄ±ldÄ± - Basit tutuldu
    # Ensure vectorstore
    if st.session_state.vectorstore is None:
        try:
            st.session_state.vectorstore = get_vectorstore()
            st.session_state.retriever = build_retriever(
                st.session_state.vectorstore,
                search_type=search_type,
                top_k=top_k,
                mmr_lambda=mmr_lambda,
            )
        except Exception as e:
            st.info("Ã–nce dosya yÃ¼kleyip indeksleyin.")
    # LLM init - Sadece OpenAI
    llm = None
    if OPENAI_API_KEY:
        llm = ChatOpenAI(
            model=openai_model, 
            temperature=0.1,
            max_tokens=1000
        )  # tool-calling destekli + maliyet optimizasyonu
    else:
        # Fallback (chain modunda Ã§alÄ±ÅŸÄ±r). Ollama kurulu deÄŸilse hata verir.
        try:
            llm = ChatOllama(model="llama3", temperature=0)
            if mode == "Agent (tools)":
                st.warning("Ajan modu iÃ§in OpenAI Ã¶nerilir; yerel modeller her zaman tool-calling desteklemez.")
        except Exception:
            llm = None

    # Init agent if needed - cevap stiline gÃ¶re yeniden oluÅŸtur
    if mode == "Agent (tools)" and st.session_state.retriever and llm:
        # Cevap stili deÄŸiÅŸtiyse agent'i yeniden oluÅŸtur
        is_short = (answer_style == "KÄ±sa ve Ã–z")
        current_style = getattr(st.session_state, 'agent_answer_style', None)
        
        if st.session_state.agent_exec is None or current_style != is_short:
            try:
                st.session_state.agent_exec = build_agent(llm, st.session_state.retriever, is_short)
                st.session_state.agent_answer_style = is_short
            except Exception as e:
                st.error(f"Agent oluÅŸturulamadÄ±: {e}")
                st.session_state.agent_exec = None

    # Chat history
    chat_container = st.container()
    with chat_container:
        history = st.session_state.chat_history_agent if mode == "Agent (tools)" else st.session_state.chat_history_chain
        for m in history:
            role = "assistant" if isinstance(m, AIMessage) else "user"
            with st.chat_message(role):
                st.markdown(m.content)

    question = st.chat_input("Sorunuzu yazÄ±nâ€¦")
    if question and st.session_state.retriever and llm:
        if mode == "RAG Chain":
            # RAG Chain modu
            st.session_state.chat_history_chain.append(HumanMessage(content=question))
            
            # Cevap stiline gÃ¶re is_short parametresini belirle
            is_short = (answer_style == "KÄ±sa ve Ã–z")
            result = answer_with_chain(llm, st.session_state.retriever, question, is_short)
            
            answer = result["answer"]
            cites = result["citations"]
            st.session_state.chat_history_chain.append(AIMessage(content=answer + ("\n\n" + cites if cites else "")))
            
            # Sohbet geÃ§miÅŸini dosyaya kaydet
            save_chat_history(st.session_state.chat_history_chain, "rag_chain")
            
            with chat_container:
                with st.chat_message("assistant"):
                    st.markdown(answer)
                    if cites:
                        st.caption(cites)
            
            # Logging removed for simplicity
        else:
            # Agent modu
            if st.session_state.agent_exec:
                st.session_state.chat_history_agent.append(HumanMessage(content=question))
                result = run_agent(st.session_state.agent_exec, question, st.session_state.chat_history_agent)
                answer = result["answer"]
                cites = result["citations"]
                st.session_state.chat_history_agent.append(AIMessage(content=answer + ("\n\n" + cites if cites else "")))
                
                # Sohbet geÃ§miÅŸini dosyaya kaydet
                save_chat_history(st.session_state.chat_history_agent, "agent")
                
                with chat_container:
                    with st.chat_message("assistant"):
                        st.markdown(answer)
                        if cites:
                            st.caption(cites)
                
                # Logging removed for simplicity
            else:
                st.error("Agent baÅŸlatÄ±lamadÄ±. RAG Chain moduna geÃ§in veya OpenAI anahtarÄ± ekleyin.")
    elif question and not llm:
        st.error("LLM baÅŸlatÄ±lamadÄ±. OpenAI anahtarÄ± ekleyin veya Ollama kurun.")

# Logs tab removed for simplicity

import os
from pathlib import Path
import streamlit as st
import pandas as pd

# Lazy imports - sadece gerektiƒüinde y√ºkle
def get_imports():
    """Lazy imports to reduce startup time"""
    try:
        from langchain_openai import ChatOpenAI
        from langchain_community.chat_models import ChatOllama
        from langchain_core.messages import AIMessage, HumanMessage
        return True, None
    except Exception as e:
        return False, str(e)

def get_rag_imports():
    """Lazy imports for RAG functionality"""
    try:
        from ingest import index_files, get_vectorstore, reset_vectorstore
        from rag_chain import build_retriever, answer_with_chain
        from agent import build_agent, run_agent
        return True, None
    except Exception as e:
        return False, str(e)

def get_config():
    """Lazy config import"""
    try:
        from config import (
            UPLOAD_DIRECTORY, PERSIST_DIRECTORY,
            OPENAI_API_KEY, DEFAULT_OPENAI_MODEL,
            SEARCH_TYPE, TOP_K, MMR_LAMBDA
        )
        from rag_chain import ensure_dirs
        return True, None
    except Exception as e:
        return False, str(e)

st.set_page_config(page_title="DocuBrain - Intelligent Document Assistant", layout="wide")
st.title("üß† DocuBrain")
st.markdown("*Turn your PDFs and Docs into an intelligent assistant.*", help="DocuBrain ile dok√ºmanlarƒ±nƒ±zƒ± akƒ±llƒ± asistanƒ±nƒ±za d√∂n√º≈üt√ºr√ºn")

# Check imports
import_success, import_error = get_imports()
if not import_success:
    st.error(f"‚ùå Import hatasƒ±: {import_error}")
    st.stop()

rag_success, rag_error = get_rag_imports()
if not rag_success:
    st.error(f"‚ùå RAG import hatasƒ±: {rag_error}")
    st.stop()

config_success, config_error = get_config()
if not config_success:
    st.error(f"‚ùå Config import hatasƒ±: {config_error}")
    st.stop()

# Import everything now that we know it works
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import AIMessage, HumanMessage
from ingest import index_files, get_vectorstore, reset_vectorstore
from rag_chain import build_retriever, answer_with_chain
from agent import build_agent, run_agent
# Logger removed for simplicity
from config import (
    UPLOAD_DIRECTORY, PERSIST_DIRECTORY,
    OPENAI_API_KEY, DEFAULT_OPENAI_MODEL,
    SEARCH_TYPE, TOP_K, MMR_LAMBDA
)
from rag_chain import ensure_dirs
from chat_storage import save_chat_history, load_chat_history, clear_chat_history, get_chat_stats

# State
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "retriever" not in st.session_state:
    st.session_state.retriever = None
# Chat history will be loaded from file storage
if "chat_history_chain" not in st.session_state:
    st.session_state.chat_history_chain = load_chat_history("rag_chain")
if "chat_history_agent" not in st.session_state:
    st.session_state.chat_history_agent = load_chat_history("agent")
if "agent_exec" not in st.session_state:
    st.session_state.agent_exec = None
if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = []
if "indexed_files" not in st.session_state:
    st.session_state.indexed_files = []

ensure_dirs(UPLOAD_DIRECTORY, PERSIST_DIRECTORY)

# Sidebar ‚Äî settings
with st.sidebar:
    st.header("‚öôÔ∏è Ayarlar")
    # Mod: OpenAI anahtarƒ± varsa varsayƒ±lan olarak Agent; yoksa RAG Chain
    mode_default = "Agent (tools)" if OPENAI_API_KEY else "RAG Chain"
    mode = st.selectbox(
        "Kullanƒ±m Modu",
        ["RAG Chain", "Agent (tools)"],
        index=1 if mode_default == "Agent (tools)" else 0,
        help="RAG Chain: Basit ve hƒ±zlƒ±. Agent: Her soruda bilgi tabanƒ± aracƒ±nƒ± kullanƒ±r (OpenAI √∂nerilir).",
    )

    # Optimized settings - kullanƒ±cƒ± ayarlamasƒ±na gerek yok
    top_k = TOP_K  # 8 - optimal chunk sayƒ±sƒ±
    search_type = SEARCH_TYPE  # "mmr" - √ße≈üitlilik i√ßin
    mmr_lambda = float(MMR_LAMBDA)  # 0.6 - optimal denge
    
    
    # Cevap uzunluƒüu se√ßimi
    st.divider()
    st.subheader("üí¨ Cevap Stili")
    answer_style = st.radio(
        "Cevap uzunluƒüu se√ßin:",
        ["Kƒ±sa ve √ñz", "Detaylƒ± ve Kapsamlƒ±"],
        index=0,
        help="Kƒ±sa: 2-3 c√ºmle, Detaylƒ±: Kapsamlƒ± a√ßƒ±klama"
    )

    st.divider()
    st.caption("LLM Se√ßimi")
    
    # Sadece OpenAI kullanƒ±labilir
    use_openai = True
    if not OPENAI_API_KEY:
        st.info("OPENAI_API_KEY bulunamadƒ±. .env dosyasƒ±yla ekleyebilirsiniz.")
    # Sadece gpt-4o-mini kullanƒ±labilir (maliyet optimizasyonu)
    openai_model = "gpt-4o-mini"
    st.info("üöÄ **GPT-4o-mini** se√ßildi (hƒ±zlƒ± + ekonomik)")
    st.caption("OpenAI GPT-4o-mini modeli kullanƒ±lƒ±yor.")

    if st.button("üóëÔ∏è Veri Tabanƒ±nƒ± Sƒ±fƒ±rla"):
        reset_vectorstore()
        st.session_state.vectorstore = None
        st.session_state.retriever = None
        st.session_state.agent_exec = None
        st.success("Chroma veritabanƒ± sƒ±fƒ±rlandƒ±.")
    

# Tabs
tab_kb, tab_chat = st.tabs(["üìÅ Knowledge Base", "üí¨ Chat"])

with tab_kb:
    st.subheader("Dosya Y√ºkle (PDF / DOCX)")
    
    # Sayfa y√ºklendiƒüinde mevcut dosyalarƒ± kontrol et
    if not st.session_state.indexed_files:
        existing_files = [p for p in UPLOAD_DIRECTORY.iterdir() if p.suffix.lower() in [".pdf", ".docx"]]
        if existing_files:
            st.session_state.indexed_files = existing_files
            # Eƒüer vectorstore varsa, dosyalar zaten indekslenmi≈ü demektir
            if st.session_state.vectorstore is None:
                try:
                    st.session_state.vectorstore = get_vectorstore()
                    st.session_state.retriever = build_retriever(
                        st.session_state.vectorstore,
                        search_type=SEARCH_TYPE,
                        top_k=TOP_K,
                        mmr_lambda=MMR_LAMBDA,
                    )
                except Exception:
                    pass  # Vectorstore yoksa yeni indeksleme gerekir
    
    files = st.file_uploader("Dosyalarƒ± se√ßin", type=["pdf", "docx"], accept_multiple_files=True)
    if files:
        saved_paths = []
        for f in files:
            save_path = UPLOAD_DIRECTORY / f.name
            with open(save_path, "wb") as out:
                out.write(f.read())
            saved_paths.append(save_path)
        st.session_state.uploaded_files = saved_paths
        st.success(f"{len(saved_paths)} dosya y√ºklendi.")
        
        # Logging removed for simplicity

    if st.button("üì• ƒ∞ndeksle (Chroma'ya ekle)"):
        path_list = [p for p in UPLOAD_DIRECTORY.iterdir() if p.suffix.lower() in [".pdf", ".docx"]]
        if not path_list:
            st.warning("√ñnce en az bir PDF/DOCX y√ºkleyin.")
        else:
            try:
                import time
                start_t = time.time()
                with st.spinner("Belgeler indeksleniyor, l√ºtfen bekleyin‚Ä¶"):
                    raw_n, chunk_n = index_files(path_list)
                elapsed = time.time() - start_t
                st.success(f"Y√ºklendi: {raw_n} belge, {chunk_n} par√ßa eklendi. ({elapsed:.1f}s)")
                st.session_state.vectorstore = get_vectorstore()
                st.session_state.retriever = build_retriever(
                    st.session_state.vectorstore,
                    search_type=search_type,
                    top_k=top_k,
                    mmr_lambda=mmr_lambda,
                )
                st.session_state.agent_exec = None  # rebuild agent next time
                st.session_state.indexed_files = path_list  # Store indexed files
                
                # Logging removed for simplicity
            except OSError as e:
                if getattr(e, 'errno', None) == 28:
                    st.error(
                        "Disk dolu: HuggingFace modeli indirilirken yer kalmadƒ±. \n\n"
                        "√á√∂z√ºm: Daha bo≈ü bir dizine cache y√∂nlendirin (√∂rn. D:):\n"
                        "- PowerShell (ge√ßici): $env:HF_HOME=\"D:\\hf_cache\"\n"
                        "- Kalƒ±cƒ±: Sistem Deƒüi≈ükeni olarak HF_HOME veya HUGGINGFACE_HUB_CACHE ekleyin.\n"
                        "- Alternatif: .env i√ßine EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2 yazƒ±n (daha k√º√ß√ºk model).\n"
                        "- Veya disk alanƒ± bo≈üaltƒ±n."
                    )
                else:
                    st.error(f"ƒ∞ndeksleme sƒ±rasƒ±nda hata: {e}")

    # Show uploaded files info
    st.divider()
    st.subheader("üìä Dosya Durumu")
    
    if st.session_state.indexed_files:
        st.success(f"‚úÖ **ƒ∞ndekslenen dosyalar:** {len(st.session_state.indexed_files)} dosya")
        
        # Tablo formatƒ±nda g√∂ster
        file_data = []
        for file_path in st.session_state.indexed_files:
            file_size = os.path.getsize(file_path) / 1024  # KB
            file_data.append({
                "Dosya Adƒ±": file_path.name,
                "Tip": file_path.suffix.upper(),
                "Boyut (KB)": f"{file_size:.1f}"
            })
        
        if file_data:
            df = pd.DataFrame(file_data)
            st.dataframe(df, width="stretch", hide_index=True)
            
            # Dosya silme se√ßeneƒüi
            st.divider()
            col1, col2 = st.columns([3, 1])
            with col1:
                selected_files = st.multiselect(
                    "Silmek istediƒüiniz dosyalarƒ± se√ßin:",
                    options=[f.name for f in st.session_state.indexed_files],
                    help="Birden fazla dosya se√ßebilirsiniz"
                )
            with col2:
                st.write("")  # Bo≈üluk i√ßin
                if st.button("üóëÔ∏è Se√ßili Dosyalarƒ± Sil", type="secondary", disabled=not selected_files):
                    # Se√ßili dosyalarƒ± sil
                    for file_name in selected_files:
                        for file_path in st.session_state.indexed_files:
                            if file_path.name == file_name:
                                try:
                                    # Dosyayƒ± fiziksel olarak sil
                                    file_path.unlink()
                                    st.success(f"‚úÖ {file_name} silindi")
                                except Exception as e:
                                    st.error(f"‚ùå {file_name} silinemedi: {e}")
                    
                    # Session state'i g√ºncelle
                    st.session_state.indexed_files = [
                        f for f in st.session_state.indexed_files 
                        if f.name not in selected_files
                    ]
                    
                    # Eƒüer t√ºm dosyalar silindiyse vectorstore'u temizle
                    if not st.session_state.indexed_files:
                        reset_vectorstore()
                        st.session_state.vectorstore = None
                        st.session_state.retriever = None
                        st.session_state.agent_exec = None
                        st.info("T√ºm dosyalar silindi. Veri tabanƒ± sƒ±fƒ±rlandƒ±.")
                    else:
                        # Kalan dosyalar varsa yeniden indeksle
                        st.warning("‚ö†Ô∏è Dosyalar silindi. Deƒüi≈üikliklerin etkili olmasƒ± i√ßin 'ƒ∞ndeksle' butonuna tƒ±klayƒ±n.")
                    
                    st.rerun()
    else:
        st.info("üìÅ Hen√ºz indekslenmi≈ü dosya yok. Dosya y√ºkleyip 'ƒ∞ndeksle' butonuna tƒ±klayƒ±n.")
    
    if st.session_state.uploaded_files:
        st.caption(f"Son y√ºkleme: {len(st.session_state.uploaded_files)} dosya")
    
    st.caption(f"üíæ Chroma klas√∂r√º: {PERSIST_DIRECTORY}")

with tab_chat:
    col1, col2 = st.columns([3, 1])
    with col1:
        st.subheader("Sohbet")
    with col2:
        if st.button("üóëÔ∏è Sohbeti Temizle", help="T√ºm sohbet ge√ßmi≈üini sil"):
            clear_chat_history()  # T√ºm sohbet ge√ßmi≈üini temizle
            st.session_state.chat_history_chain = []
            st.session_state.chat_history_agent = []
            st.success("‚úÖ Sohbet ge√ßmi≈üi temizlendi!")
            st.rerun()
    
    # Sohbet y√∂netimi
    if st.session_state.chat_history_chain or st.session_state.chat_history_agent:
        st.divider()
        st.subheader("üìä Sohbet Y√∂netimi")
        
        # Sohbet ge√ßmi≈üini g√∂ster
        all_chats = []
        for i, msg in enumerate(st.session_state.chat_history_chain):
            if isinstance(msg, HumanMessage):
                all_chats.append({
                    "Mod": "RAG Chain",
                    "Soru": msg.content[:50] + "..." if len(msg.content) > 50 else msg.content,
                    "Tip": "Soru",
                    "Index": f"chain_{i}"
                })
            elif isinstance(msg, AIMessage):
                all_chats.append({
                    "Mod": "RAG Chain", 
                    "Soru": msg.content[:50] + "..." if len(msg.content) > 50 else msg.content,
                    "Tip": "Cevap",
                    "Index": f"chain_{i}"
                })
        
        for i, msg in enumerate(st.session_state.chat_history_agent):
            if isinstance(msg, HumanMessage):
                all_chats.append({
                    "Mod": "Agent",
                    "Soru": msg.content[:50] + "..." if len(msg.content) > 50 else msg.content,
                    "Tip": "Soru",
                    "Index": f"agent_{i}"
                })
            elif isinstance(msg, AIMessage):
                all_chats.append({
                    "Mod": "Agent",
                    "Soru": msg.content[:50] + "..." if len(msg.content) > 50 else msg.content,
                    "Tip": "Cevap", 
                    "Index": f"agent_{i}"
                })
        
        if all_chats:
            # Sohbet listesi
            df = pd.DataFrame(all_chats)
            st.dataframe(df, width="stretch", hide_index=True)
            
            # Sohbet silme se√ßeneƒüi
            col1, col2 = st.columns([3, 1])
            with col1:
                selected_chats = st.multiselect(
                    "Silmek istediƒüiniz sohbetleri se√ßin:",
                    options=[f"{chat['Mod']} - {chat['Tip']} ({chat['Index']})" for chat in all_chats],
                    help="Birden fazla sohbet se√ßebilirsiniz"
                )
            with col2:
                st.write("")  # Bo≈üluk i√ßin
                if st.button("üóëÔ∏è Se√ßili Sohbetleri Sil", type="secondary", disabled=not selected_chats):
                    # Se√ßili sohbetleri sil
                    for selected in selected_chats:
                        # Index'i parse et
                        if "chain_" in selected:
                            index = int(selected.split("chain_")[1].split(")")[0])
                            if index < len(st.session_state.chat_history_chain):
                                st.session_state.chat_history_chain.pop(index)
                        elif "agent_" in selected:
                            index = int(selected.split("agent_")[1].split(")")[0])
                            if index < len(st.session_state.chat_history_agent):
                                st.session_state.chat_history_agent.pop(index)
                    
                    # Dosyaya kaydet
                    save_chat_history(st.session_state.chat_history_chain, "rag_chain")
                    save_chat_history(st.session_state.chat_history_agent, "agent")
                    
                    st.success("‚úÖ Se√ßili sohbetler silindi!")
                    st.rerun()
    else:
        st.info("üìÅ Hen√ºz sohbet ge√ßmi≈üi yok. Sohbet ba≈ülatƒ±n!")
    # Ensure vectorstore
    if st.session_state.vectorstore is None:
        try:
            st.session_state.vectorstore = get_vectorstore()
            st.session_state.retriever = build_retriever(
                st.session_state.vectorstore,
                search_type=search_type,
                top_k=top_k,
                mmr_lambda=mmr_lambda,
            )
        except Exception as e:
            st.info("√ñnce dosya y√ºkleyip indeksleyin.")
    # LLM init - Sadece OpenAI
    llm = None
    if OPENAI_API_KEY:
        llm = ChatOpenAI(
            model=openai_model, 
            temperature=0.1,
            max_tokens=1000
        )  # tool-calling destekli + maliyet optimizasyonu
    else:
        # Fallback (chain modunda √ßalƒ±≈üƒ±r). Ollama kurulu deƒüilse hata verir.
        try:
            llm = ChatOllama(model="llama3", temperature=0)
            if mode == "Agent (tools)":
                st.warning("Ajan modu i√ßin OpenAI √∂nerilir; yerel modeller her zaman tool-calling desteklemez.")
        except Exception:
            llm = None

    # Init agent if needed - cevap stiline g√∂re yeniden olu≈ütur
    if mode == "Agent (tools)" and st.session_state.retriever and llm:
        # Cevap stili deƒüi≈ütiyse agent'i yeniden olu≈ütur
        is_short = (answer_style == "Kƒ±sa ve √ñz")
        current_style = getattr(st.session_state, 'agent_answer_style', None)
        
        if st.session_state.agent_exec is None or current_style != is_short:
            try:
                st.session_state.agent_exec = build_agent(llm, st.session_state.retriever, is_short)
                st.session_state.agent_answer_style = is_short
            except Exception as e:
                st.error(f"Agent olu≈üturulamadƒ±: {e}")
                st.session_state.agent_exec = None

    # Chat history
    chat_container = st.container()
    with chat_container:
        history = st.session_state.chat_history_agent if mode == "Agent (tools)" else st.session_state.chat_history_chain
        for m in history:
            role = "assistant" if isinstance(m, AIMessage) else "user"
            with st.chat_message(role):
                st.markdown(m.content)

    question = st.chat_input("Sorunuzu yazƒ±n‚Ä¶")
    if question and st.session_state.retriever and llm:
        if mode == "RAG Chain":
            # RAG Chain modu
            st.session_state.chat_history_chain.append(HumanMessage(content=question))
            
            # Cevap stiline g√∂re is_short parametresini belirle
            is_short = (answer_style == "Kƒ±sa ve √ñz")
            result = answer_with_chain(llm, st.session_state.retriever, question, is_short)
            
            answer = result["answer"]
            cites = result["citations"]
            st.session_state.chat_history_chain.append(AIMessage(content=answer + ("\n\n" + cites if cites else "")))
            
            # Sohbet ge√ßmi≈üini dosyaya kaydet
            save_chat_history(st.session_state.chat_history_chain, "rag_chain")
            
            with chat_container:
                with st.chat_message("assistant"):
                    st.markdown(answer)
                    if cites:
                        st.caption(cites)
            
            # Logging removed for simplicity
        else:
            # Agent modu
            if st.session_state.agent_exec:
                st.session_state.chat_history_agent.append(HumanMessage(content=question))
                result = run_agent(st.session_state.agent_exec, question, st.session_state.chat_history_agent)
                answer = result["answer"]
                cites = result["citations"]
                st.session_state.chat_history_agent.append(AIMessage(content=answer + ("\n\n" + cites if cites else "")))
                
                # Sohbet ge√ßmi≈üini dosyaya kaydet
                save_chat_history(st.session_state.chat_history_agent, "agent")
                
                with chat_container:
                    with st.chat_message("assistant"):
                        st.markdown(answer)
                        if cites:
                            st.caption(cites)
                
                # Logging removed for simplicity
            else:
                st.error("Agent ba≈ülatƒ±lamadƒ±. RAG Chain moduna ge√ßin veya OpenAI anahtarƒ± ekleyin.")
    elif question and not llm:
        st.error("LLM ba≈ülatƒ±lamadƒ±. OpenAI anahtarƒ± ekleyin veya Ollama kurun.")

# Logs tab removed for simplicity
